{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0dc97d",
   "metadata": {},
   "source": [
    "# Earth Engine Explore\n",
    "\n",
    "Explore possible explanatory and response variables for fire risk modeling across the Amazon\n",
    "\n",
    "\n",
    "Most code borrowed from: https://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_getPixels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5185e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import concurrent\n",
    "import ee\n",
    "import google\n",
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "\n",
    "from google.api_core import retry\n",
    "from google.protobuf import json_format\n",
    "from IPython.display import Image\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('animation', html='html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111002ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 54\n",
    "RNG = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc60085",
   "metadata": {},
   "source": [
    "# Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksolvik-misc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials, _ = google.auth.default()\n",
    "ee.Initialize(project=PROJECT, opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d66b1",
   "metadata": {},
   "source": [
    "# Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d77f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: ALPHAEARTH_EMBEDDINGS\n",
    "ALPHAEARTH_EMBEDDINGS = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n",
    "\n",
    "# The years from which to sample for alpha earth\n",
    "YEARS = np.arange(2023, 2024)\n",
    "# Target variable: MB Fire annual burned area\n",
    "MB_FIRE = ee.Image('projects/mapbiomas-public/assets/brazil/fire/collection4_1/mapbiomas_fire_collection41_annual_burned_v1')\n",
    "\n",
    "# Region of interest to sample from\n",
    "ROI = gpd.read_file('../data/Limites_RAISG_2025/Lim_Raisg.shp')\n",
    "\n",
    "# Number of areas to sample\n",
    "N_SAMPLE = 2000\n",
    "\n",
    "# Set final scale\n",
    "FINAL_SCALE = 30 # in Meters\n",
    "# Set patch size\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "# How much to keep as validation\n",
    "VALIDATION_RATIO=0.2\n",
    "\n",
    "# For model, input and output bands\n",
    "INPUT_BANDS = ['{}{:02d}'.format('A', i) for i in range(64)] + ['burned_area_{}'.format(y) for y in range(2000, 2024)]\n",
    "OUTPUT_BANDS = ['burned_area_2024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_points(roi: gpd.GeoDataFrame, n_sample: int, rng: np.random.Generator)->np.array:\n",
    "  \"\"\"Get random points within region of interest.\"\"\"\n",
    "  sample_df = roi.sample_points(n_sample, rng=rng).geometry.explode().get_coordinates()\n",
    "  sample_df.index = np.arange(sample_df.shape[0])\n",
    "  return sample_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27714441",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precompute some inputs based on params\n",
    "# A random sample of N locations in the ROI as a list of GeoJSON points.\n",
    "SAMPLE_POINTS  = sample_random_points(ROI, N_SAMPLE, RNG)\n",
    "\n",
    "# Make a projection to discover the scale in degrees.\n",
    "PROJ_AE = ee.Projection('EPSG:4326').atScale(FINAL_SCALE)\n",
    "PROJ_AE_DICT = PROJ_AE.getInfo()\n",
    "# Get scales out of the transform.\n",
    "SCALE_X = PROJ_AE_DICT['transform'][0]\n",
    "SCALE_Y = -PROJ_AE_DICT['transform'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce6673",
   "metadata": {},
   "source": [
    "## Image retrieval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4561550",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@retry.Retry()\n",
    "def compute_patch(coords, image, patch_size, scale_x, scale_y, band_sel=None):\n",
    "  \"\"\"Compute a patch of pixel, with upper-left corner defined by the coords.\"\"\"\n",
    "\n",
    "  # Make a request object.\n",
    "  request = {\n",
    "      'expression':image,\n",
    "      'fileFormat': 'NPY',\n",
    "      'grid': {\n",
    "          'dimensions': {\n",
    "              'width': patch_size,\n",
    "              'height':patch_size\n",
    "          },\n",
    "          'affineTransform': {\n",
    "              'scaleX': scale_x,\n",
    "              'shearX': 0,\n",
    "              'translateX': coords[0],\n",
    "              'shearY': 0,\n",
    "              'scaleY': scale_y,\n",
    "              'translateY': coords[1]\n",
    "          },\n",
    "          'crsCode': 'EPSG:4326',\n",
    "      },\n",
    "  }\n",
    "\n",
    "  if not band_sel is None:\n",
    "    request['bandIds'] = band_sel\n",
    "\n",
    "  return np.load(io.BytesIO(ee.data.computePixels(request)))\n",
    "\n",
    "def serialize_example(structured_array):\n",
    "  \"\"\"Convert structured numpy array to serliazed tf.Example proto\"\"\"\n",
    "  return array_to_example(structured_array).SerializeToString()\n",
    "\n",
    "def array_to_example(structured_array):\n",
    "  \"\"\"\"Convert structured numpy array to tf.Example proto.\"\"\"\n",
    "  feature = {}\n",
    "  for f in structured_array.dtype.names:\n",
    "    feature[f] = tf.train.Feature(\n",
    "        float_list = tf.train.FloatList(\n",
    "            value = structured_array[f].flatten()))\n",
    "  return tf.train.Example(\n",
    "      features = tf.train.Features(feature = feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e51bc",
   "metadata": {},
   "source": [
    "# Option 1: Execute using simple concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeaf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# executor = concurrent.futures.ThreadPoolExecutor(max_workers=10)\n",
    "\n",
    "# writer = tf.io.TFRecordWriter(OUTPUT_FILE, 'GZIP')\n",
    "\n",
    "for year in YEARS:\n",
    "    year = int(year)\n",
    "    ae_year_mean = (ALPHAEARTH_EMBEDDINGS\n",
    "               .filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "               .mosaic()\n",
    "               .setDefaultProjection(PROJ_AE)\n",
    "               .reduceResolution('mean')\n",
    "               )\n",
    "    mb_year = MB_FIRE\n",
    "\n",
    "    joined_img = ae_year_mean.addBands(mb_year)\n",
    "\n",
    "#     future_to_image = {\n",
    "#         executor.submit(compute_patch, [point.x, point.y], joined_img, PATCH_SIZE, SCALE_X, SCALE_Y):\n",
    "#             'ALL_{}'.format(index) for index, point in SAMPLE.iterrows()\n",
    "#     }\n",
    "\n",
    "#     arrays = ()\n",
    "#     types = []\n",
    "#     for future in concurrent.futures.as_completed(future_to_image):\n",
    "#       image_id = future_to_image[future]\n",
    "#       try:\n",
    "#           np_array = future.result()\n",
    "#           example_proto = array_to_example(np_array)\n",
    "#           writer.write(example_proto.SerializeToString())\n",
    "#           writer.flush()\n",
    "#       except Exception as e:\n",
    "#           print(e)\n",
    "#           pass\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d125f",
   "metadata": {},
   "source": [
    "# Option 2: Using beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c15c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2528b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEComputePatch(beam.DoFn):\n",
    "    def setup(self):\n",
    "        ee.Initialize(project=PROJECT, opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "\n",
    "    def process(self, coords, image, patch_size, scale_x, scale_y, band_sel=None):\n",
    "        \"\"\"Compute a patch of pixel, with upper-left corner defined by the coords.\"\"\"\n",
    "\n",
    "        # Make a request object.\n",
    "        request = {\n",
    "            'expression':image,\n",
    "            'fileFormat': 'NPY',\n",
    "            'grid': {\n",
    "                'dimensions': {\n",
    "                    'width': patch_size,\n",
    "                    'height':patch_size\n",
    "                },\n",
    "                'affineTransform': {\n",
    "                    'scaleX': scale_x,\n",
    "                    'shearX': 0,\n",
    "                    'translateX': coords[0],\n",
    "                    'shearY': 0,\n",
    "                    'scaleY': scale_y,\n",
    "                    'translateY': coords[1]\n",
    "                },\n",
    "                'crsCode': 'EPSG:4326',\n",
    "            },\n",
    "        }\n",
    "\n",
    "        if not band_sel is None:\n",
    "            request['bandIds'] = band_sel\n",
    "\n",
    "        yield np.load(io.BytesIO(ee.data.computePixels(request)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(element, n_partitions) -> int:\n",
    "  weights = [1 - VALIDATION_RATIO, VALIDATION_RATIO]\n",
    "  return random.choices([0, 1], weights)[0]\n",
    "\n",
    "beam_options = PipelineOptions([], direct_num_workers=4, direct_running_mode='multi_processing')\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as pipeline:\n",
    "  training_data, validation_data = (\n",
    "      pipeline\n",
    "      | 'Create points' >> beam.Create(SAMPLE_POINTS)\n",
    "      | 'Get patch' >> beam.ParDo(EEComputePatch(), joined_img, PATCH_SIZE, SCALE_X, SCALE_Y)\n",
    "      | 'Serialize' >> beam.Map(serialize_example)\n",
    "      | 'Split dataset' >> beam.Partition(split_dataset, 2)\n",
    "  )\n",
    "\n",
    "  training_data | 'Write training data' >> beam.io.WriteToTFRecord(\n",
    "      '../data/beam/training', file_name_suffix='.tfrecord.gz'\n",
    "  )\n",
    "  validation_data | 'Write validation data' >> beam.io.WriteToTFRecord(\n",
    "      '../data/beam/validation', file_name_suffix='.tfrecord.gz'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0bd9b7",
   "metadata": {},
   "source": [
    "# Parse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the size and shape of patches expected by the model.\n",
    "FEATURES = joined_img.bandNames().getInfo()\n",
    "KERNEL_SHAPE = [PATCH_SIZE, PATCH_SIZE]\n",
    "COLUMNS = [\n",
    "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
    "]\n",
    "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"Parse a serialized example.\"\"\"\n",
    "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset('../data/beam/training-00001-of-00008.tfrecord.gz', compression_type='GZIP')\n",
    "dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85707834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in dataset:\n",
    "#   rgb = np.stack([\n",
    "#       data['A00'].numpy(),\n",
    "#       data['A01'].numpy(),\n",
    "#       data['A02'].numpy()], axis=2)\n",
    "#   plt.imshow(rgb+0.5)\n",
    "#   plt.show()\n",
    "#   plt.imshow(data['burned_area_2024'])\n",
    "#   plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da912eac",
   "metadata": {},
   "source": [
    "# Parse with option to augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.augment_inputs = tf.keras.layers.RandomFlip(\n",
    "        mode=\"horizontal_and_vertical\", seed=seed)\n",
    "    self.augment_labels = tf.keras.layers.RandomFlip(\n",
    "        mode=\"horizontal_and_vertical\", seed=seed)\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    inputs = {name: self.augment_inputs(v) for name, v in inputs.items()}\n",
    "    labels = self.augment_labels(labels)\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def parse_tfrecord(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
    "\n",
    "\n",
    "def to_tuple(inputs):\n",
    "  return (\n",
    "      {name: inputs[name] for name in INPUT_BANDS},\n",
    "      inputs[OUTPUT_BANDS[0]]\n",
    "      # tf.one_hot(tf.cast(inputs[OUTPUT_BANDS[0]], tf.uint8), )\n",
    "  )\n",
    "\n",
    "\n",
    "def get_dataset(pattern, batch_size, shuffle=True):\n",
    "  dataset = tf.data.Dataset.list_files(pattern).interleave(\n",
    "      lambda filename: tf.data.TFRecordDataset(filename, compression_type='GZIP'))\n",
    "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  dataset = dataset.map(to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  dataset = dataset.cache()\n",
    "  if shuffle:\n",
    "    dataset = dataset.shuffle(512)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "# Create the training and validation datasets.\n",
    "training_dataset = get_dataset('../data/beam/training-*.tfrecord.gz', 8).map(Augment(), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_dataset = get_dataset('../data/beam/validation-*.tfrecord.gz', 1, shuffle=False)\n",
    "\n",
    "# Inspect the first element from the training dataset.\n",
    "for inputs, outputs in training_dataset.take(1):\n",
    "  print(\"inputs:\")\n",
    "  for name, values in inputs.items():\n",
    "    print(f\"  {name}: {values.dtype.name} {values.shape}\")\n",
    "  print(f\"outputs: {outputs.dtype.name} {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b78ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_shape):\n",
    "    inputs = keras.Input(shape=[None, None, len(INPUT_BANDS)])\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    for filters in [64, 128]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_mlp(input_shape):\n",
    "    inputs = keras.Input(shape=[None, None, len(INPUT_BANDS)])\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    # x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_multi_scale_mlp_head(input_shape, hidden=128):\n",
    "    inputs = keras.Input(shape=[None, None, len(INPUT_BANDS)])\n",
    "\n",
    "    # --- scale 1 (original resolution) ---\n",
    "    s1 = layers.Dense(hidden, activation=\"gelu\")(inputs)\n",
    "\n",
    "    # --- scale 2 (128x128) ---\n",
    "    s2 = layers.AveragePooling2D(pool_size=2)(inputs)\n",
    "    s2 = layers.Dense(hidden, activation=\"gelu\")(s2)\n",
    "    s2 = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(s2)\n",
    "\n",
    "    # --- scale 3 (64x64) ---\n",
    "    s3 = layers.AveragePooling2D(pool_size=4)(inputs)\n",
    "    s3 = layers.Dense(hidden, activation=\"gelu\")(s3)\n",
    "    s3 = layers.UpSampling2D(size=4, interpolation=\"bilinear\")(s3)\n",
    "\n",
    "    # Fuse\n",
    "    fused = layers.Concatenate()([s1, s2, s3])\n",
    "    fused = layers.LayerNormalization()(fused)\n",
    "    fused = layers.Dense(hidden, activation=\"gelu\")(fused)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(fused)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mlp([PATCH_SIZE, PATCH_SIZE, len(INPUT_BANDS)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input dictionary layers.\n",
    "inputs_dict = {\n",
    "    name: tf.keras.Input(shape=(None, None, 1), name=name)\n",
    "    for name in INPUT_BANDS\n",
    "}\n",
    "\n",
    "concat = tf.keras.layers.Concatenate()(list(inputs_dict.values()))\n",
    "new_model = tf.keras.Model(inputs=inputs_dict, outputs=model(concat))\n",
    "# print(new_model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025),\n",
    "    loss=\"Dice\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryIoU(target_class_ids=[1]),\n",
    "    ]\n",
    "    )\n",
    "\n",
    "checkpoint_filepath = './checkpoint.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    patience=10)\n",
    "\n",
    "new_model.fit(\n",
    "    training_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=25,\n",
    "    callbacks=[model_checkpoint_callback, early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('checkpoint.model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_masks = np.array([b[1][i].numpy() for b in validation_dataset for i in range(batch[1].shape[0])])\n",
    "valid_burn_lastyear= np.array([b[0]['burned_area_2023'][i].numpy() + b[0]['burned_area_2022'][i].numpy() for b in validation_dataset for i in range(batch[1].shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f86a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = new_model.predict(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cdcc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f1_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))\n",
    "print(recall_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))\n",
    "print(precision_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))\n",
    "print(jaccard_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f1_score(valid_masks.flatten()>0.5, out.flatten()>0.99))\n",
    "print(recall_score(valid_masks.flatten()>0.5, out.flatten()>0.99))\n",
    "print(precision_score(valid_masks.flatten()>0.5, out.flatten()>0.99))\n",
    "print(jaccard_score(valid_masks.flatten()>0.5, out.flatten()>0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for batch in validation_dataset:\n",
    "    for i in range(batch[1].shape[0]):\n",
    "        if (batch[1][i].numpy()>0.5).sum()>0 or (out[j]>0.99).sum()>0:\n",
    "            plt.imshow(batch[1][i].numpy())\n",
    "            plt.title(j)\n",
    "            plt.show()\n",
    "            plt.imshow(out[j]>0.99)\n",
    "            plt.title('{}-pred'.format(j))\n",
    "            plt.show()\n",
    "        j+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aic-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
