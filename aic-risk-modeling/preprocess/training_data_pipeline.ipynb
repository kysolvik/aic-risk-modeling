{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0dc97d",
   "metadata": {},
   "source": [
    "# Training data pipeline\n",
    "\n",
    "\n",
    "Most code borrowed from: https://colab.research.google.com/github/google/earthengine-community/blob/master/guides/linked/Earth_Engine_training_patches_getPixels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5185e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.api_core import retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111002ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 54\n",
    "RNG = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d66b1",
   "metadata": {},
   "source": [
    "# Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d77f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The year we're predicting on (and target data will be sampled from)\n",
    "TARGET_YEAR = 2024\n",
    "\n",
    "# Region of interest to sample from\n",
    "ROI = gpd.read_file('../data/Limites_RAISG_2025/Lim_Raisg.shp')\n",
    "\n",
    "# Number of areas to sample\n",
    "N_SAMPLE = 2000\n",
    "\n",
    "# Set final scale\n",
    "FINAL_SCALE = 500 # in Meters\n",
    "\n",
    "# Set patch size\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "# How much to keep as validation\n",
    "VALIDATION_RATIO=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97529c4",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_points(roi: gpd.GeoDataFrame, n_sample: int, rng: np.random.Generator)->np.array:\n",
    "  \"\"\"Get random points within region of interest.\"\"\"\n",
    "  sample_df = roi.sample_points(n_sample, rng=rng).geometry.explode().get_coordinates()\n",
    "  sample_df.index = np.arange(sample_df.shape[0])\n",
    "  return sample_df.values\n",
    "\n",
    "def array_to_example(structured_array):\n",
    "  \"\"\"\"Convert structured numpy array to tf.Example proto.\"\"\"\n",
    "  feature = {}\n",
    "  for f in structured_array.dtype.names:\n",
    "    feature[f] = tf.train.Feature(\n",
    "        float_list = tf.train.FloatList(\n",
    "            value = structured_array[f].flatten()))\n",
    "  return tf.train.Example(\n",
    "      features = tf.train.Features(feature = feature))\n",
    "\n",
    "def serialize_example(structured_array):\n",
    "  \"\"\"Convert structured numpy array to serliazed tf.Example proto\"\"\"\n",
    "  return array_to_example(structured_array).SerializeToString()\n",
    "\n",
    "def split_dataset(element, n_partitions) -> int:\n",
    "  weights = [1 - VALIDATION_RATIO, VALIDATION_RATIO]\n",
    "  return random.choices([0, 1], weights)[0]\n",
    "\n",
    "def points_to_geeflow_csv(sample_points, out_csv_path):\n",
    "  lon = sample_points[:,0]\n",
    "  lat = sample_points[:,1]\n",
    "  out_df = pd.DataFrame({\n",
    "    'lat': lat,\n",
    "    'lon': lon,\n",
    "    'label': 0,\n",
    "    'split': 'train',\n",
    "    'index': range(lon.shape[0])\n",
    "  }\n",
    "  ).set_index('index')\n",
    "\n",
    "  # Shuffle order\n",
    "  out_df = out_df.sample(frac=1).reset_index(drop=True)\n",
    "  out_df.index.names = ['index']\n",
    "\n",
    "  num_train = round(out_df.shape[0]*(1-VALIDATION_RATIO))\n",
    "  out_df.loc[num_train:, 'split'] = 'val'\n",
    "\n",
    "  out_df.to_csv(out_csv_path)\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27714441",
   "metadata": {},
   "source": [
    "# Custom Beam DoFn for gathering EE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2528b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEComputePatch(beam.DoFn):\n",
    "    \"\"\"DoFn() for computing EE patch\n",
    "    \n",
    "    config (dict): Dictionary containing configuration settings \n",
    "        in the following key:value pairs:\n",
    "            project_id (str): Google Cloud project id\n",
    "            patch_size (int): Patch size, in pixels, of output chips\n",
    "            scale (float): Final scale, in m\n",
    "            target_year (int): Year of prediction\n",
    "            target_key (str): Name of target data, corresponds to key in self.prep_dict\n",
    "            inputs_keys (list): Names of input data, correspond to keys in self.prep_dict\n",
    "            proj (str): Projection, e.g. \"EPSG:4326\"\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.prep_dict = {\n",
    "            'embeddings': self._prep_embeddings,\n",
    "            'mcd64': self._prep_mcd64,\n",
    "            'mb_fire': self._prep_mb_burned_area\n",
    "        }\n",
    "\n",
    "    def setup(self):\n",
    "        print(f\"Initializing Earth Engine for project: {self.config['project_id']}\")\n",
    "        ee.Initialize(project=self.config['project_id'], opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "\n",
    "        # Set some params\n",
    "        self.proj = ee.Projection(self.config['proj']).atScale(self.config['scale'])\n",
    "        self.proj_dict = self.proj.getInfo()\n",
    "        self.scale_x = self.proj_dict['transform'][0]\n",
    "        self.scale_y = -self.proj_dict['transform'][4]\n",
    "\n",
    "        # Setup Earth Engine image object with all target bands\n",
    "        inputs_list = [\n",
    "            self.prep_dict[k](self.config['target_year']-1)\n",
    "            for k in self.config['inputs_keys']\n",
    "        ]\n",
    "        outputs_list = [self.prep_dict[self.config['target_key']](self.config['target_year'])]\n",
    "        full_list = inputs_list + outputs_list\n",
    "        # Get original band names, with system indices prepended (toBands() adds)\n",
    "        band_names = [\n",
    "            bn \n",
    "            for image in full_list\n",
    "            for bn in image.bandNames().getInfo()\n",
    "        ]\n",
    "\n",
    "        # Final prepped image\n",
    "        self.prepped_image = ee.ImageCollection(inputs_list + outputs_list).toBands().rename(band_names)\n",
    "\n",
    "    @retry.Retry()\n",
    "    def process(self, coords):\n",
    "        \"\"\"Compute a patch of pixel, with upper-left corner defined by the coords.\"\"\"\n",
    "\n",
    "        # Make a request object.\n",
    "        request = {\n",
    "            'expression':self.prepped_image,\n",
    "            'fileFormat': 'NPY',\n",
    "            'grid': {\n",
    "                'dimensions': {\n",
    "                    'width': self.config['patch_size'],\n",
    "                    'height':self.config['patch_size']\n",
    "                },\n",
    "                'affineTransform': {\n",
    "                    'scaleX': self.scale_x,\n",
    "                    'shearX': 0,\n",
    "                    'translateX': coords[0],\n",
    "                    'shearY': 0,\n",
    "                    'scaleY': self.scale_y,\n",
    "                    'translateY': coords[1]\n",
    "                },\n",
    "                'crsCode': 'EPSG:4326',\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield np.load(io.BytesIO(ee.data.computePixels(request)))\n",
    "    \n",
    "    def _prep_embeddings(self, year):\n",
    "        return (\n",
    "            ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n",
    "            .filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "            .mosaic()\n",
    "            .setDefaultProjection(self.proj)\n",
    "            .reduceResolution('mean', maxPixels=500)\n",
    "            )\n",
    "\n",
    "    def _prep_mcd64(self, year):\n",
    "        return (\n",
    "            ee.ImageCollection('MODIS/061/MCD64A1')\n",
    "            .select('BurnDate')\n",
    "            .filter(ee.Filter.calendarRange(year, year, 'year'))\n",
    "            .min()\n",
    "            )\n",
    "\n",
    "    def _prep_mb_burned_area(self, year):\n",
    "        return (\n",
    "            ee.Image('projects/mapbiomas-public/assets/brazil/fire/collection4_1/mapbiomas_fire_collection41_annual_burned_v1')\n",
    "            .select(['burned_area_{}'.format(year)])\n",
    "            .reduceResolution('mean', maxPixels=500)\n",
    "            )\n",
    "\n",
    "    def _prep_default(self, year):\n",
    "        \"\"\"Example prep method\"\"\"\n",
    "        return (\n",
    "            ee.ImageCollection()\n",
    "            .mean()\n",
    "            .reduceResolution('mean', maxPixels=500)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d125f",
   "metadata": {},
   "source": [
    "# Run beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random sample of N locations in the ROI as a list of GeoJSON points.\n",
    "SAMPLE_POINTS  = sample_random_points(ROI, N_SAMPLE, RNG)\n",
    "\n",
    "# Write to csv compatible with geeflow\n",
    "points_to_geeflow_csv(SAMPLE_POINTS, './amazon.csv')\n",
    "\n",
    "beam_options = PipelineOptions([], direct_num_workers=8, direct_running_mode='multi_processing')\n",
    "\n",
    "config_dict = {\n",
    "    'project_id': 'ksolvik-misc',\n",
    "    'patch_size': PATCH_SIZE,\n",
    "    'scale': FINAL_SCALE,\n",
    "    'target_year': TARGET_YEAR,\n",
    "    'target_key': 'mcd64',\n",
    "    'inputs_keys': ['embeddings', 'mb_fire'],\n",
    "    'proj': 'EPSG:4326'\n",
    "}\n",
    "\n",
    "with beam.Pipeline(options=beam_options) as pipeline:\n",
    "  training_data, validation_data = (\n",
    "      pipeline\n",
    "      | 'Create points' >> beam.Create(SAMPLE_POINTS)\n",
    "      | 'Get patch' >> beam.ParDo(EEComputePatch(config_dict))\n",
    "      | 'Serialize' >> beam.Map(serialize_example)\n",
    "      | 'Split dataset' >> beam.Partition(split_dataset, 2)\n",
    "  )\n",
    "\n",
    "  training_data | 'Write training data' >> beam.io.WriteToTFRecord(\n",
    "      '../data/beamMODIS/training', file_name_suffix='.tfrecord.gz'\n",
    "  )\n",
    "  validation_data | 'Write validation data' >> beam.io.WriteToTFRecord(\n",
    "      '../data/beamMODIS/validation', file_name_suffix='.tfrecord.gz'\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aic-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
