{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd0dc97d",
      "metadata": {
        "id": "dd0dc97d"
      },
      "source": [
        "# 00. Train Model\n",
        "\n",
        "Helper notebook for training fire risk model on colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kysolvik/aic-risk-modeling.git\n",
        "!cd /content/aic-risk-modeling/; git checkout vertex-train; pip install -e .\n",
        "!pip install --upgrade tensorflow-metadata\n"
      ],
      "metadata": {
        "id": "-2A5GkyBn8ZD"
      },
      "id": "-2A5GkyBn8ZD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade tensorflow-metadata\n"
      ],
      "metadata": {
        "id": "V2nrhFbnAHa-"
      },
      "id": "V2nrhFbnAHa-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "Ee2CcHNyzy5c"
      },
      "id": "Ee2CcHNyzy5c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart runtime\n",
        "from IPython import get_ipython\n",
        "\n",
        "get_ipython().kernel.do_shutdown(restart=True)"
      ],
      "metadata": {
        "id": "g4ftIeNGAS3V"
      },
      "id": "g4ftIeNGAS3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5185e4",
      "metadata": {
        "id": "ef5185e4"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pickle\n",
        "import json\n",
        "import aic_risk_modeling.train as train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111002ff",
      "metadata": {
        "id": "111002ff"
      },
      "outputs": [],
      "source": [
        "SEED = 54\n",
        "RNG = np.random.default_rng(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb3d66b1",
      "metadata": {
        "id": "eb3d66b1"
      },
      "source": [
        "# Set params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d77f23",
      "metadata": {
        "id": "45d77f23"
      },
      "outputs": [],
      "source": [
        "# GCS Dir holding schema and tfrecords\n",
        "GCS_DATA_DIR = 'gs://aic-fire-amazon/results_2024_5k/'\n",
        "# glob match path for tfrecords\n",
        "TFRECORD_PATTERN = '*.tfrecord.gz'\n",
        "# Set patch size\n",
        "PATCH_SIZE = 128\n",
        "\n",
        "# For model, input and output bands\n",
        "INPUT_BANDS = (\n",
        "    ['{}{:02d}'.format('A', i) for i in range(64)]\n",
        "    + ['burned_area_2023']\n",
        ")\n",
        "OUTPUT_BAND = 'BurnDate'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da912eac",
      "metadata": {
        "id": "da912eac"
      },
      "source": [
        "# Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849f9b27",
      "metadata": {
        "id": "849f9b27"
      },
      "outputs": [],
      "source": [
        "training_pattern = os.path.join(GCS_DATA_DIR, 'training-{}'.format(TFRECORD_PATTERN))\n",
        "validation_pattern = os.path.join(GCS_DATA_DIR, 'validation-{}'.format(TFRECORD_PATTERN))\n",
        "\n",
        "schema = train.load_schema_from_gcs(GCS_DATA_DIR)\n",
        "feature_spec = train.build_features_dict(schema, patch_size=PATCH_SIZE)\n",
        "\n",
        "print(\"Example features:\")\n",
        "for i, f in enumerate(list(feature_spec.keys())[:20]):\n",
        "    print(i + 1, f)\n",
        "\n",
        "training_ds = train.dataset_from_gcs(training_pattern, feature_spec,\n",
        "                               input_bands=[k for k in feature_spec.keys() if k not in ['lat','lon','id', OUTPUT_BAND]],\n",
        "                               output_bands=[OUTPUT_BAND],\n",
        "                               batch_size=4,\n",
        "                               shuffle_buffer=64,\n",
        "                               cache=False)\n",
        "validation_ds = train.dataset_from_gcs(validation_pattern, feature_spec,\n",
        "                               input_bands=[k for k in feature_spec.keys() if k not in ['lat','lon','id', OUTPUT_BAND]],\n",
        "                               output_bands=[OUTPUT_BAND],\n",
        "                               batch_size=4,\n",
        "                               shuffle=False,\n",
        "                               cache=False)\n",
        "for inputs, labels in training_ds.take(1):\n",
        "    print(\"Batch inputs keys:\", list(inputs.keys()))\n",
        "    print(\"Label shape:\", labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8af8926",
      "metadata": {
        "id": "e8af8926"
      },
      "outputs": [],
      "source": [
        "model = train.get_mlp([PATCH_SIZE, PATCH_SIZE, len(INPUT_BANDS)])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0f9e2e",
      "metadata": {
        "id": "7e0f9e2e"
      },
      "outputs": [],
      "source": [
        "# Define the input dictionary layers.\n",
        "inputs_dict = {\n",
        "    name: tf.keras.Input(shape=(None, None, 1), name=name)\n",
        "    for name in INPUT_BANDS\n",
        "}\n",
        "\n",
        "concat = tf.keras.layers.Concatenate()(list(inputs_dict.values()))\n",
        "new_model = tf.keras.Model(inputs=inputs_dict, outputs=model(concat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9230ba67",
      "metadata": {
        "id": "9230ba67"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025),\n",
        "    loss=\"Dice\",\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryIoU(target_class_ids=[1]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "checkpoint_filepath = './checkpoint.model.keras'\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    patience=5)\n",
        "\n",
        "new_model.fit(\n",
        "    training_ds,\n",
        "    validation_data=validation_ds,\n",
        "    epochs=20,\n",
        "    steps_per_epoch=500,\n",
        "    validation_steps=100,\n",
        "    callbacks=[model_checkpoint_callback, early_stopping_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd0598c",
      "metadata": {
        "id": "dfd0598c"
      },
      "outputs": [],
      "source": [
        "new_model = tf.keras.models.load_model('checkpoint.model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_ds = validation_ds.take(50)\n",
        "valid_masks = np.array(list(small_ds.map(lambda inputs, mask: mask).as_numpy_iterator()))\n",
        "valid_masks = valid_masks.reshape(-1, 128, 128)\n"
      ],
      "metadata": {
        "id": "6mbG8cAeo7JS"
      },
      "id": "6mbG8cAeo7JS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5d9e57c",
      "metadata": {
        "id": "d5d9e57c"
      },
      "outputs": [],
      "source": [
        "valid_masks = np.array([b[1][i].numpy() for b in validation_ds for i in range(b[1].shape[0])])\n",
        "valid_burn_lastyear= np.array([b[0]['burned_area_2023'][i].numpy() for b in validation_ds for i in range(b[1].shape[0])])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list = []\n",
        "mask_list  =[]\n",
        "for images, labels in small_ds:\n",
        "    preds = new_model.predict(images)\n",
        "    pred_list.append(preds)\n",
        "    mask_list.append(labels)"
      ],
      "metadata": {
        "id": "DfE8GfdsqYBA"
      },
      "id": "DfE8GfdsqYBA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_masks = np.array(mask_list).reshape(-1, 128, 128)\n",
        "out = np.array(pred_list).reshape(-1, 128, 128)"
      ],
      "metadata": {
        "id": "LS1kDD71qxuQ"
      },
      "id": "LS1kDD71qxuQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cSk5tn-imRVi"
      },
      "id": "cSk5tn-imRVi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f86a70",
      "metadata": {
        "id": "29f86a70"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, jaccard_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "847c8388",
      "metadata": {
        "id": "847c8388"
      },
      "outputs": [],
      "source": [
        "# out = new_model.predict(validation_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9cdcc70",
      "metadata": {
        "id": "a9cdcc70"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f1_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))\n",
        "print(recall_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))\n",
        "print(precision_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))\n",
        "print(jaccard_score(valid_masks.flatten()>0.5, valid_burn_lastyear.flatten()>0.5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(out > 0.5).sum()"
      ],
      "metadata": {
        "id": "WcaFMZblmXO8"
      },
      "id": "WcaFMZblmXO8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9586add8",
      "metadata": {
        "id": "9586add8"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f1_score(valid_masks.flatten()>0.5, out.flatten()>0.9))\n",
        "print(recall_score(valid_masks.flatten()>0.5, out.flatten()>0.5))\n",
        "print(precision_score(valid_masks.flatten()>0.5, out.flatten()>0.5))\n",
        "print(jaccard_score(valid_masks.flatten()>0.5, out.flatten()>0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b84bb8",
      "metadata": {
        "id": "b9b84bb8"
      },
      "outputs": [],
      "source": [
        "def visualize_risk_predict(input_batch, target_batch, output_i, batch_i, suptitle, cutoff=0.5):\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, axs = plt.subplots(2,2)\n",
        "    fig.suptitle(suptitle)\n",
        "    # Embeddings\n",
        "    rgb = np.stack([\n",
        "        input_batch['A01'][batch_i].numpy(),\n",
        "        input_batch['A16'][batch_i].numpy(),\n",
        "        input_batch['A09'][batch_i].numpy()], axis=2)\n",
        "    # shift\n",
        "    vmin=-0.3\n",
        "    vmax=0.3\n",
        "    rgb = (rgb - vmin)/(vmax - vmin)\n",
        "    axs.flatten()[0].imshow(rgb)\n",
        "    axs.flatten()[0].set_title('Embeddings')\n",
        "\n",
        "    # 2022 burn\n",
        "    axs.flatten()[1].imshow(input_batch['burned_area_2023'][batch_i].numpy() > 0.5)\n",
        "    axs.flatten()[1].set_title('Burned area 2023')\n",
        "\n",
        "    # Prediction\n",
        "    axs.flatten()[2].imshow(output_i)\n",
        "    axs.flatten()[2].set_title('Predicted burned area 2024')\n",
        "\n",
        "    # 2023 burn (target)\n",
        "    axs.flatten()[3].imshow(target_batch[batch_i].numpy()>cutoff)\n",
        "    axs.flatten()[3].set_title('Actual burned area 2024')\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(valid_masks.shape[0]):\n",
        "    visualize_risk_predict(\n",
        "        input_batch = inputs,\n",
        "        target_batch = valid_masks,\n",
        "        output_i = out[i],\n",
        "        batch_i=i,\n",
        "        suptitle='Image {}'.format(i),\n",
        "        cutoff=0"
      ],
      "metadata": {
        "id": "AyocVBrGrOdD"
      },
      "id": "AyocVBrGrOdD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = 0\n",
        "for batch in small_ds:\n",
        "    out = new_model.predict(batch[0])\n",
        "    for i in range(batch[1].shape[0]):\n",
        "        if (batch[1][i].numpy()>0.5).sum()>0 or (out[i]>0.5).sum()>0:\n",
        "            visualize_risk_predict(\n",
        "                input_batch = batch[0],\n",
        "                target_batch = batch[1],\n",
        "                output_i = out[i],\n",
        "                batch_i=i,\n",
        "                suptitle='Image {}'.format(j),\n",
        "                cutoff=0.5\n",
        "            )\n",
        "        j+=1\n",
        "        if j > 100:\n",
        "           break"
      ],
      "metadata": {
        "id": "M1BpullUrzNL"
      },
      "id": "M1BpullUrzNL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3dfded8",
      "metadata": {
        "id": "c3dfded8"
      },
      "outputs": [],
      "source": [
        "j = 0\n",
        "for batch in validation_ds:\n",
        "    for i in range(batch[1].shape[0]):\n",
        "        if (batch[1][i].numpy()>0.5).sum()>0 or (out[j]>0.5).sum()>0:\n",
        "            visualize_risk_predict(\n",
        "                input_batch = batch[0],\n",
        "                target_batch = batch[1],\n",
        "                output_i = out[j],\n",
        "                batch_i=i,\n",
        "                suptitle='Image {}'.format(j),\n",
        "                cutoff=0.5\n",
        "            )\n",
        "        j+=1\n",
        "        if j > 100:\n",
        "           break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}